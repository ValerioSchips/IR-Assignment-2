{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code Referennce: https://www.analyticsvidhya.com/blog/2020/08/information-retrieval-using-word2vec-based-vector-space-model/#h2_7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from gensim.models.word2vec import Word2Vec as W2V\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import string\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DatasetLoad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = W2V()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select on which dataset we want to work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_col_names = ['stem_text', 'doc_text', 'lem_text']\n",
    "df_col_select = df_col_names[2]\n",
    "df_query_col_names = ['cl_q', 'stem_q', 'lem_q']\n",
    "df_query_col_select = df_query_col_names[2]\n",
    "test_dim = 0.2\n",
    "print(\"TEXT: \" + df_col_select + \"\\nQUERY: \" + df_query_col_select + \"\\nTEST_DIM: \" + str(test_dim))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure the model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_model_size = 100\n",
    "wv_model_m_c = 0\n",
    "wv_model_win = 5\n",
    "wv_model_type = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataframes Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"Docs/\"\n",
    "luc_retr = path+\"raw_dev_Lucene_retrievals.csv\"\n",
    "g_truth_rank = path + \"dev_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_cl = \"ProcDocs/\" + \"/Split_\"+str(test_dim)+\"/\"\n",
    "docs_test_path = path_cl +\"docs_test.csv\"\n",
    "docs_train_path = path_cl +\"docs_train.csv\"\n",
    "queries_test_path = path_cl +\"queries_test.csv\"\n",
    "queries_train_path = path_cl +\"queries_train.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"ProcDocs/W2V_c/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_train_df = pd.read_csv(docs_train_path)\n",
    "docs_test_df = pd.read_csv(docs_test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_train_df = pd.read_csv(queries_train_path)\n",
    "queries_test_df = pd.read_csv(queries_test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "luc_retr_df = pd.read_csv(luc_retr)\n",
    "g_truth_r = pd.read_csv(g_truth_rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W2V Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining corpus and queries for training\n",
    "combined_training=pd.concat([docs_train_df.rename(columns={df_col_select:'text'})['text'],\\\n",
    "                             queries_train_df.rename(columns={df_query_col_select:'text'})['text'],\\\n",
    "                                 queries_test_df.rename(columns={df_query_col_select:'text'})['text']])\\\n",
    ".sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=[]\n",
    "for i in tqdm(combined_training):\n",
    "    train_data.append(i.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training a word2vec model from the given data set\n",
    "w2v_model = W2V(tqdm(train_data), size=wv_model_size, min_count=wv_model_m_c, window=wv_model_win, sg=wv_model_type, workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = \"cl_text\"\n",
    "os.mkdir(model_path + dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.save(model_path + dir +\"/mod.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = W2V.load(model_path + \"SG_lem_text_lem_q/mod.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(w2v_model.wv.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Vectors for the test DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(534)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function returning vector reperesentation of a document\n",
    "def get_embedding_w2v(doc_tokens, wv_m_size = wv_model_size, model = w2v_model):\n",
    "    embeddings = []\n",
    "    if len(doc_tokens)<1:\n",
    "        return np.zeros(wv_m_size)\n",
    "    else:\n",
    "        for tok in doc_tokens:\n",
    "            if tok in model.wv.vocab:\n",
    "                embeddings.append(model.wv.word_vec(tok))\n",
    "            else:\n",
    "                continue\n",
    "        #if len(embeddings) == 0:\n",
    "         #   return np.zeros(wv_m_size)\n",
    "        # mean the vectors of individual words to get the vector of the document\n",
    "        return np.mean(embeddings, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting Word2Vec Vectors for Testing Corpus and Queries\n",
    "docs_test_df['vector']=docs_test_df[df_col_select].progress_apply(lambda x :get_embedding_w2v(str(x).split()))\n",
    "queries_test_df['vector']=queries_test_df[df_query_col_select].progress_apply(lambda x :get_embedding_w2v(str(x).split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Re-rank documents for a query\n",
    "def reorder_docs(q_num, lucene_res, top_N = 10):\n",
    "  # generating vector\n",
    "  vector = queries_test_df[queries_test_df.Query_number == q_num]['vector'].values[0]\n",
    "\n",
    "  #selectin docs to order\n",
    "  tmp_docs_df = docs_test_df[docs_test_df.doc_number.isin(lucene_res)].copy()\n",
    "  \n",
    "  # ranking documents\n",
    "  documents=tmp_docs_df.copy()\n",
    "  documents['similarity']=documents['vector'].apply(lambda x: cosine_similarity(np.array(vector).reshape(1, -1),np.array(x).reshape(1, -1)).item())\n",
    "  documents.sort_values(by='similarity',ascending=False,inplace=True)\n",
    "  return documents.head(top_N).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Re-rank documents for a query and returns the Recall@K\n",
    "def get_reorder_recall(q_num, recall_at=10):\n",
    "    lucene_query_doc = list(luc_retr_df[luc_retr_df.Query_number == q_num].doc_number) # Select the document related to that query\n",
    "\n",
    "    tmp_compare_result = g_truth_r[g_truth_r.Query_number == q_num].reset_index() # Select the benchmark slice\n",
    "\n",
    "    n_of_ret = len(tmp_compare_result)\n",
    "    if recall_at == 0: recall_at = n_of_ret \n",
    "\n",
    "    ordered_res = reorder_docs(q_num, lucene_query_doc, n_of_ret).reset_index() # Get W2V similarity\n",
    "    \n",
    "    tmp_compare_result = tmp_compare_result.iloc[:recall_at].reset_index() # Select only slice to compare\n",
    "    n_of_ret = len(tmp_compare_result)\n",
    "\n",
    "    #Calculate recall\n",
    "    count_correct = 0\n",
    "    for x in range(n_of_ret):\n",
    "        if tmp_compare_result.loc[x, 'doc_number'] == ordered_res.loc[x, 'doc_number']:\n",
    "            count_correct += 1\n",
    "    if count_correct == 0:\n",
    "        return 0\n",
    "    return count_correct/n_of_ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_test_df['score'] = queries_test_df.Query_number.progress_apply(lambda x: get_reorder_recall(x,recall_at=5))\n",
    "queries_test_df['score'].mean()*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_test_df['score'] = queries_test_df.Query_number.progress_apply(lambda x: get_reorder_recall(x,recall_at=10))\n",
    "queries_test_df['score'].mean()*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_test_df['score'] = queries_test_df.Query_number.progress_apply(lambda x: get_reorder_recall(x,recall_at=20))\n",
    "queries_test_df['score'].mean()*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for calculating average precision for a query\n",
    "def average_precision(qid,qvector, only_relevant=False):\n",
    "  \n",
    "  # Getting the ground truth and document vectors\n",
    "  qresult=g_truth_r.loc[g_truth_r['Query_number']==qid,['doc_number','label']]\n",
    "  n_of_rel = len(qresult)\n",
    "  if only_relevant == True: n_of_rel = len(qresult[qresult['label'] == 1])\n",
    "  qcorpus=docs_test_df[docs_test_df['doc_number'].isin(list(qresult['doc_number']))].reset_index(drop=True)\n",
    "  qcorpus = qcorpus[['doc_number','vector']]\n",
    "  \n",
    "  qresult=pd.merge(qresult,qcorpus,on='doc_number')\n",
    "  \n",
    "  # Ranking documents for the query\n",
    "  qresult['similarity']=qresult['vector'].apply(lambda x: cosine_similarity(np.array(qvector).reshape(1, -1),np.array(x).reshape(1, -1)).item())\n",
    "  qresult.sort_values(by='similarity',ascending=False,inplace=True)\n",
    "  # Taking Top 10 documents for the evaluation\n",
    "  ranking=qresult.head(n_of_rel)['label'].values\n",
    "  \n",
    "  # Calculating precision\n",
    "  precision=[]\n",
    "  for i in range(1,n_of_rel):\n",
    "    if ranking[i-1]:\n",
    "      precision.append(np.sum(ranking[:i])/i)\n",
    "  \n",
    "  # If no relevant document in list then return 0\n",
    "  if precision==[]:\n",
    "    return 0\n",
    "\n",
    "  return np.mean(precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avg Pec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_test_df['AP']=queries_test_df.progress_apply(lambda x: average_precision(x['Query_number'],x['vector'], only_relevant=True),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding Mean Average Precision\n",
    "print('Mean Average Precision=>',queries_test_df['AP'].mean()*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the Dataset Load stage before and load the function under: \"Compute Vectors for the test Dataset\" and \"Evaluate\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates models with different vector size and calculates the recall and average precision to determine which parameter fits better the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining corpus and queries for training\n",
    "combined_training=pd.concat([docs_train_df.rename(columns={df_col_select:'text'})['text'],\\\n",
    "                             queries_train_df.rename(columns={df_query_col_select:'text'})['text'],\\\n",
    "                                 queries_test_df.rename(columns={df_query_col_select:'text'})['text']])\\\n",
    ".sample(frac=1).reset_index(drop=True)\n",
    "train_data=[]\n",
    "for i in tqdm(combined_training):\n",
    "    train_data.append(i.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list_d = dict() #Models Dictioary\n",
    "recall_dict = dict() #Scores Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_test = [100,300,500,800,1000] #[3,5,10,15] #[100,300,500,800,1000] #Different size or window to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the models, set x on the parameter you want to optimize\n",
    "for x in tqdm(size_test):\n",
    "    w2v_model = W2V(train_data, size=x, min_count=wv_model_m_c, window=wv_model_win, sg=wv_model_type, workers=8)\n",
    "    model_list_d[x] = w2v_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculates the recall and average precision for all the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in tqdm(size_test):\n",
    "    vect_size = x\n",
    "    docs_test_df['vector']=docs_test_df[df_col_select].apply(lambda y :get_embedding_w2v(str(y).split(), wv_m_size=int(vect_size), model = model_list_d[x]))\n",
    "    queries_test_df['vector']=queries_test_df[df_query_col_select].apply(lambda y :get_embedding_w2v(str(y).split(), wv_m_size= int(vect_size), model = model_list_d[x]))\n",
    "    queries_test_df['score'] = queries_test_df.Query_number.apply(lambda y: get_reorder_recall(y,recall_at=5))\n",
    "    r5 = queries_test_df['score'].mean()\n",
    "    queries_test_df['score'] = queries_test_df.Query_number.apply(lambda y: get_reorder_recall(y,recall_at=10))\n",
    "    r10 = queries_test_df['score'].mean()\n",
    "    queries_test_df['score'] = queries_test_df.Query_number.apply(lambda y: get_reorder_recall(y,recall_at=20))\n",
    "    r20 = queries_test_df['score'].mean()\n",
    "    queries_test_df['AP']=queries_test_df.apply(lambda x: average_precision(x['Query_number'],x['vector'], only_relevant=True),axis=1)\n",
    "    avg_p = queries_test_df['AP'].mean()\n",
    "    recall_dict[x] = [r5,r10,r20, avg_p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the recall dict\n",
    "for aa, bb in recall_dict.items():\n",
    "    print(str(aa)+\":\")\n",
    "    for z in bb:\n",
    "        print(str(z*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avg Precision Plot\n",
    "ordered_avg_dict = []\n",
    "for x in size_test:\n",
    "    ordered_avg_dict.append(recall_dict[x][3]*100)\n",
    "plt.plot(size_test, ordered_avg_dict)\n",
    "plt.yticks(np.arange(70,80,1))\n",
    "plt.xlabel(\"Vec Size\")\n",
    "plt.ylabel(\"Avg Prec %\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall Plot\n",
    "ordered_recall_dict_5 = []\n",
    "ordered_recall_dict_10 = []\n",
    "ordered_recall_dict_mean = []\n",
    "for x in size_test:\n",
    "    ordered_recall_dict_5.append(recall_dict[x][0]*100)\n",
    "    ordered_recall_dict_10.append(recall_dict[x][1]*100)\n",
    "    ordered_recall_dict_mean.append(np.mean([recall_dict[x][0]*100, recall_dict[x][1]*100]))\n",
    "plt.plot(size_test, ordered_recall_dict_5)\n",
    "plt.plot(size_test, ordered_recall_dict_10)\n",
    "plt.plot(size_test, ordered_recall_dict_mean)\n",
    "plt.yticks(np.arange(0,11,1))\n",
    "plt.xlabel(\"Vec Size\")\n",
    "plt.ylabel(\"Rec %\")\n",
    "plt.legend([\"Rec@5\", \"Rec@10\", \"Rec@Mean\"], loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in tqdm(size_test):\n",
    "    end_fix = model_path+\"param_opt_win\"\n",
    "    end_fix = end_fix+ \"/W2V_model_\"+str(x)\n",
    "    os.mkdir(end_fix)\n",
    "    model_list_d[x].save(end_fix + \"/mod.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in tqdm(size_test):\n",
    "    end_fix = model_path+\"param_opt_lem_text_lem_q\"\n",
    "    end_fix = end_fix+ \"/W2V_model_\"+str(x)\n",
    "    model_list_d[x] = W2V.load(end_fix + \"/mod.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TSNE - Plot a query in the vector space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def cluster(keys, model, topn = 15):    \n",
    "    embedding_clusters = []\n",
    "    word_clusters = []\n",
    "    for word in keys:\n",
    "        embeddings = []\n",
    "        words = []\n",
    "        for similar_word, _ in model.most_similar(word, topn=topn):\n",
    "            words.append(similar_word)\n",
    "            embeddings.append(model[similar_word])\n",
    "        embedding_clusters.append(embeddings)\n",
    "        word_clusters.append(words)\n",
    "\n",
    "    embedding_clusters = np.array(embedding_clusters)\n",
    "    n, m, k = embedding_clusters.shape\n",
    "    tsne_model_en_2d = TSNE(perplexity=15, n_components=2, init='pca', n_iter=3500, random_state=32)\n",
    "    embeddings_en_2d = np.array(tsne_model_en_2d.fit_transform(embedding_clusters.reshape(n * m, k))).reshape(n, m, 2)\n",
    "    return [embeddings_en_2d, word_clusters]\n",
    "keys = \"sherwin williams phone number\".split()\n",
    "clust = cluster(keys=keys, model=w2v_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "#% matplotlib inline\n",
    "\n",
    "\n",
    "def tsne_plot_similar_words(title, labels, embedding_clusters, word_clusters, a, filename=None):\n",
    "    plt.figure(figsize=(16*5/3, 9*5/3))\n",
    "    colors = cm.rainbow(np.linspace(0, 1, len(labels)))\n",
    "    for label, embeddings, words, color in zip(labels, embedding_clusters, word_clusters, colors):\n",
    "        x = embeddings[:, 0]\n",
    "        y = embeddings[:, 1]\n",
    "        plt.scatter(x, y, c=color, alpha=a, label=label, s = 50)\n",
    "        for i, word in enumerate(words):\n",
    "            plt.annotate(word, alpha=0.8, xy=(x[i], y[i]), xytext=(5, 2),\n",
    "                         textcoords='offset points', ha='right', va='bottom', size=14)\n",
    "    plt.legend(loc=4)\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    if filename:\n",
    "        plt.savefig(filename, format='png', dpi=250, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "tsne_plot_similar_words('Similar words to a query', keys, clust[0], clust[1], 0.7,\n",
    "                        'similar_words.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Data Reordering - Executed on the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_doc = pd.read_csv(\"ProcDocs/test_data/test_data_doc.csv\")\n",
    "test_data_query = pd.read_csv(\"ProcDocs/test_data/test_data_query.csv\")\n",
    "test_data_lucene = pd.read_csv(\"ProcDocs/test_data/test_data_lucene.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_doc_complete = test_data_doc.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_doc_complete = test_data_doc_complete.append(docs_train_df[docs_train_df.doc_number.isin(list(test_data_lucene.doc_number))][['doc_number','doc_text','lem_text']]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining corpus and queries for training\n",
    "combined_training=pd.concat([docs_train_df.rename(columns={df_col_select:'text'})['text'],\\\n",
    "                             queries_train_df.rename(columns={df_query_col_select:'text'})['text'],\\\n",
    "                                 queries_test_df.rename(columns={df_query_col_select:'text'})['text'],\\\n",
    "                                     test_data_doc.rename(columns={df_col_select:'text'})['text'],\\\n",
    "                                         test_data_query.rename(columns={df_query_col_select:'text'})['text'],\\\n",
    "                                 ])\\\n",
    ".sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=[]\n",
    "for i in tqdm(combined_training):\n",
    "    train_data.append(i.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training a word2vec model from the given data set\n",
    "w2v_model = W2V(tqdm(train_data), size=1000, min_count=0, window=10, sg=1, workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = \"test_data\"\n",
    "os.mkdir(model_path + dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.save(model_path + dir +\"/mod.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function returning vector reperesentation of a document\n",
    "def get_embedding_w2v_test_data(doc_tokens, wv_m_size = 1000, model = w2v_model):\n",
    "    embeddings = []\n",
    "    if len(doc_tokens)<1:\n",
    "        return np.zeros(wv_m_size)\n",
    "    else:\n",
    "        for tok in doc_tokens:\n",
    "            if tok in model.wv.vocab:\n",
    "                embeddings.append(model.wv.word_vec(tok))\n",
    "            else:\n",
    "                continue\n",
    "        # mean the vectors of individual words to get the vector of the document\n",
    "        return np.mean(embeddings, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting Word2Vec Vectors for Testing Corpus and Queries\n",
    "test_data_doc_complete['vector']=test_data_doc_complete[df_col_select].progress_apply(lambda x :get_embedding_w2v(str(x).split()))\n",
    "test_data_query['vector']=test_data_query[df_query_col_select].progress_apply(lambda x :get_embedding_w2v(str(x).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Re-rank documents for a query\n",
    "def reorder_docs_test_data(q_num, top_N = 10):\n",
    "  lucene_docs_list = list(test_data_lucene[test_data_lucene.Query_number == q_num].doc_number)\n",
    "  # generating vector\n",
    "  vector = test_data_query[test_data_query.Query_number == q_num]['vector'].values[0]\n",
    "\n",
    "  #selectin docs to order\n",
    "  tmp_docs_df = test_data_doc_complete[test_data_doc_complete.doc_number.isin(lucene_docs_list)].copy()\n",
    "  \n",
    "  # ranking documents\n",
    "  documents=tmp_docs_df.copy()\n",
    "  documents['similarity']=documents['vector'].apply(lambda x: cosine_similarity(np.array(vector).reshape(1, -1),np.array(x).reshape(1, -1)).item())\n",
    "  documents.sort_values(by='similarity',ascending=False,inplace=True)\n",
    "  return documents.head(top_N).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensionality_reduction_rank = pd.DataFrame(columns=['Query_number', 'doc_number'])\n",
    "for q_num in list(test_data_query.Query_number.unique()):\n",
    "    result = reorder_docs_test_data(q_num)\n",
    "    for x in range(0, len(result)):\n",
    "        tmp = pd.DataFrame([[q_num, result.iloc[x]['doc_number']]], columns=['Query_number', 'doc_number'])\n",
    "        dimensionality_reduction_rank = dimensionality_reduction_rank.append(tmp).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensionality_reduction_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensionality_reduction_rank.to_csv('dimensionality_reduction_rank.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "19649b6d477f04954267d7dfcc1e3219afd53992c8847ec6a09d5cd5145b7914"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
