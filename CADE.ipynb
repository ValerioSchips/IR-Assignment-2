{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code Referennce: https://www.analyticsvidhya.com/blog/2020/08/information-retrieval-using-word2vec-based-vector-space-model/#h2_7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from gensim.models.word2vec import Word2Vec as W2V\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import string\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DatasetLoad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = W2V()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select on which dataset we want to work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_col_names = ['stem_text', 'doc_text', 'lem_text']\n",
    "df_col_select = df_col_names[2]\n",
    "df_query_col_names = ['cl_q', 'stem_q', 'lem_q']\n",
    "df_query_col_select = df_query_col_names[2]\n",
    "test_dim = 0.2\n",
    "print(\"TEXT: \" + df_col_select + \"\\nQUERY: \" + df_query_col_select + \"\\nTEST_DIM: \" + str(test_dim))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure the model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_model_size = 1000\n",
    "wv_model_m_c = 0\n",
    "wv_model_win = 10\n",
    "wv_model_type = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataframes Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"Docs/\"\n",
    "luc_retr = path+\"raw_dev_Lucene_retrievals.csv\"\n",
    "g_truth_rank = path + \"dev_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_cl = \"ProcDocs/\" + \"/Split_\"+str(test_dim)+\"/\"\n",
    "docs_test_path = path_cl +\"docs_test.csv\"\n",
    "docs_train_path = path_cl +\"docs_train.csv\"\n",
    "queries_test_path = path_cl +\"queries_test.csv\"\n",
    "queries_train_path = path_cl +\"queries_train.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"ProcDocs/W2V/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_train_df = pd.read_csv(docs_train_path)\n",
    "docs_test_df = pd.read_csv(docs_test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_train_df = pd.read_csv(queries_train_path)\n",
    "queries_test_df = pd.read_csv(queries_test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "luc_retr_df = pd.read_csv(luc_retr)\n",
    "g_truth_r = pd.read_csv(g_truth_rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W2V Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining corpus and queries for training\n",
    "combined_training=pd.concat([docs_train_df.rename(columns={df_col_select:'text'})['text'],\\\n",
    "                             queries_train_df.rename(columns={df_query_col_select:'text'})['text'],\\\n",
    "                                 queries_test_df.rename(columns={df_query_col_select:'text'})['text']])\\\n",
    "                             .sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the compass file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('compass.txt',\"w+\")\n",
    "f.close()\n",
    "with open('compass.txt', 'a') as f:\n",
    "    for x in combined_training:\n",
    "        f.write(x)\n",
    "        f.write('\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dataframe with query_num and doc_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_query = pd.merge(luc_retr_df, queries_test_df, how=\"right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_list = list(complete_query.Query_number.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_dicts = dict()\n",
    "file_name_list = list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save each file for slice training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in query_list:\n",
    "    file_name = \"CADE/\"+str(x)+\".txt\"\n",
    "    f = open(file_name,\"w+\")\n",
    "    f.close()\n",
    "    docs_list = list(complete_query[complete_query.Query_number == x].doc_number.unique())\n",
    "    with open(file_name, 'a') as f:\n",
    "        for y in docs_list: \n",
    "            f.write(docs_test_df[docs_test_df.doc_number == y][df_col_select].item())\n",
    "            f.write('\\n')\n",
    "    file_name_list.append(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cade.cade import CADE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate the model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aligner = CADE(size=wv_model_size, min_count = wv_model_m_c, workers=8, sg=0, window=wv_model_win)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the compass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aligner.train_compass(\"compass.txt\", overwrite=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train each slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_number_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in tqdm(file_name_list):\n",
    "    slice_dicts[int(x.split('.')[0].split('/')[1])] = aligner.train_slice(x)\n",
    "    query_number_list.append(int(x.split('.')[0].split('/')[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Dataset Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function returning vector reperesentation of a document\n",
    "def get_embedding_w2v(doc_tokens, model, wv_m_size = wv_model_size):\n",
    "    embeddings = []\n",
    "    if len(doc_tokens)<1:\n",
    "        return np.zeros(wv_m_size)\n",
    "    else:\n",
    "        for tok in doc_tokens:\n",
    "            if tok in model.wv.vocab:\n",
    "                embeddings.append(model.wv[tok])\n",
    "            else:\n",
    "                continue\n",
    "                embeddings.append(np.random.rand(wv_m_size))\n",
    "        # mean the vectors of individual words to get the vector of the document\n",
    "        return np.mean(embeddings, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting Word2Vec Vectors for Testing Corpus and Queries\n",
    "docs_test_df['vector']=docs_test_df[['doc_number', df_col_select]].progress_apply(lambda x :get_embedding_w2v(str(x[1]).split(), model = aligner.compass), axis = 1)\n",
    "queries_test_df['vector']=queries_test_df[['Query_number', df_query_col_select]].progress_apply(lambda x :get_embedding_w2v(str(x[1]).split(), model = slice_dicts[x[0]]), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Re-rank documents for a query\n",
    "def reorder_docs(q_num, lucene_res, top_N = 10):\n",
    "  # generating vector\n",
    "  \n",
    "  vector = queries_test_df[queries_test_df.Query_number == q_num]['vector'].values[0]\n",
    "\n",
    "  #selectin docs to order\n",
    "  tmp_docs_df = docs_test_df[docs_test_df.doc_number.isin(lucene_res)].copy()\n",
    "  tmp_docs_df['vector'] = tmp_docs_df[['doc_number', df_col_select]].apply(lambda x :get_embedding_w2v(str(x[1]).split(), model = slice_dicts[q_num]), axis = 1).copy()\n",
    "  #docs_test_df[docs_test_df.doc_number.isin(lucene_res)].copy()\n",
    "  \n",
    "  # ranking documents\n",
    "  documents=tmp_docs_df.copy()\n",
    "  documents['similarity']=documents['vector'].apply(lambda x: cosine_similarity(np.array(vector).reshape(1, -1),np.array(x).reshape(1, -1)).item())\n",
    "  documents.sort_values(by='similarity',ascending=False,inplace=True)\n",
    "  return documents.head(top_N).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Re-rank documents for a query and returns the Recall@K\n",
    "def get_reorder_recall(q_num, recall_at=10):\n",
    "    lucene_query_doc = list(luc_retr_df[luc_retr_df.Query_number == q_num].doc_number) # Select the document related to that query\n",
    "\n",
    "    tmp_compare_result = g_truth_r[g_truth_r.Query_number == q_num].reset_index() # Select the benchmark slice\n",
    "\n",
    "    n_of_ret = len(tmp_compare_result)\n",
    "    if recall_at == 0: recall_at = n_of_ret \n",
    "\n",
    "    ordered_res = reorder_docs(q_num, lucene_query_doc, n_of_ret).reset_index() # Get W2V similarity\n",
    "    \n",
    "    tmp_compare_result = tmp_compare_result.iloc[:recall_at].reset_index() # Select only slice to compare\n",
    "    n_of_ret = len(tmp_compare_result)\n",
    "\n",
    "    #Calculate recall\n",
    "    count_correct = 0\n",
    "    for x in range(n_of_ret):\n",
    "        if tmp_compare_result.loc[x, 'doc_number'] == ordered_res.loc[x, 'doc_number']:\n",
    "            count_correct += 1\n",
    "    if count_correct == 0:\n",
    "        return 0\n",
    "    return count_correct/n_of_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_test_df['score'] = queries_test_df.Query_number.progress_apply(lambda x: get_reorder_recall(x,recall_at=5))\n",
    "queries_test_df['score'].mean()*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_test_df['score'] = queries_test_df.Query_number.progress_apply(lambda x: get_reorder_recall(x,recall_at=10))\n",
    "queries_test_df['score'].mean()*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_test_df['score'] = queries_test_df.Query_number.progress_apply(lambda x: get_reorder_recall(x,recall_at=20))\n",
    "queries_test_df['score'].mean()*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for calculating average precision for a query\n",
    "def average_precision(qid,qvector, only_relevant=False):\n",
    "  \n",
    "  # Getting the ground truth and document vectors\n",
    "  qresult=g_truth_r.loc[g_truth_r['Query_number']==qid,['doc_number','label']]\n",
    "  n_of_rel = len(qresult)\n",
    "  if only_relevant == True: n_of_rel = len(qresult[qresult['label'] == 1])\n",
    "  qcorpus=docs_test_df[docs_test_df['doc_number'].isin(list(qresult['doc_number']))].reset_index(drop=True)\n",
    "  qcorpus['vector'] = qcorpus[['doc_number', df_col_select]].apply(lambda x :get_embedding_w2v(str(x[1]).split(), model = slice_dicts[qid]), axis = 1).copy()\n",
    "  qcorpus = qcorpus[['doc_number','vector']]\n",
    "  \n",
    "  qresult=pd.merge(qresult,qcorpus,on='doc_number')\n",
    "  \n",
    "  # Ranking documents for the query\n",
    "  qresult['similarity']=qresult['vector'].apply(lambda x: cosine_similarity(np.array(qvector).reshape(1, -1),np.array(x).reshape(1, -1)).item())\n",
    "  qresult.sort_values(by='similarity',ascending=False,inplace=True)\n",
    "  # Taking Top 10 documents for the evaluation\n",
    "  ranking=qresult.head(n_of_rel)['label'].values\n",
    "  \n",
    "  # Calculating precision\n",
    "  precision=[]\n",
    "  for i in range(1,n_of_rel):\n",
    "    if ranking[i-1]:\n",
    "      precision.append(np.sum(ranking[:i])/i)\n",
    "  \n",
    "  # If no relevant document in list then return 0\n",
    "  if precision==[]:\n",
    "    return 0\n",
    "\n",
    "  return np.mean(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_test_df['AP']=queries_test_df.progress_apply(lambda x: average_precision(x['Query_number'],x['vector'], only_relevant=True),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding Mean Average Precision\n",
    "print('Mean Average Precision=>',queries_test_df['AP'].mean()*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "19649b6d477f04954267d7dfcc1e3219afd53992c8847ec6a09d5cd5145b7914"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
