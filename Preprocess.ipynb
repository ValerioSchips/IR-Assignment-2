{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os.path\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize regex tokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "# create English stop words list\n",
    "en_stop = set(stopwords.words('english'))\n",
    "# Create p_stemmer of class PorterStemmer\n",
    "p_stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DatasetLoad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"Docs/\"\n",
    "path_cl = \"ProcDocs/\"\n",
    "docs = path +\"all_docs.csv\"\n",
    "queries = path +\"dev_queries.csv\"\n",
    "save_docs = path_cl +\"all_docs_1.csv\"\n",
    "save_queries = path_cl +\"dev_queries_1.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(path_cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_df = pd.read_csv(docs)\n",
    "docs_df.doc_text = docs_df.doc_text.apply(str)\n",
    "docs_df = docs_df.drop_duplicates('doc_number').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_df = pd.read_csv(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lucene = pd.read_csv('Docs/raw_dev_Lucene_retrievals.csv')\n",
    "g_thruth = pd.read_csv('Docs/dev_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecy only relevant docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_docs = list(g_thruth['doc_number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_df = docs_df[docs_df.doc_number.isin(list_docs)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of english Contractions\n",
    "contractions_dict = { \"ain't\": \"are not\",\"'s\":\" is\",\"aren't\": \"are not\",\"can't\": \"can not\",\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\"could've\": \"could have\",\"couldn't\": \"could not\",\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\"doesn't\": \"does not\",\"don't\": \"do not\",\"hadn't\": \"had not\",\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\"haven't\": \"have not\",\"he'd\": \"he would\",\"he'd've\": \"he would have\",\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he will have\",\"how'd\": \"how did\",\"how'd'y\": \"how do you\",\"how'll\": \"how will\",\"i'd\": \"i would\",\n",
    "\"i'd've\": \"i would have\",\"i'll\": \"i will\",\"i'll've\": \"i will have\",\"i'm\": \"i am\",\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\"it'd\": \"it would\",\"it'd've\": \"it would have\",\"it'll\": \"it will\",\"it'll've\": \"it will have\",\n",
    "\"let's\": \"let us\",\"ma'am\": \"madam\",\"mayn't\": \"may not\",\"might've\": \"might have\",\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\"must've\": \"must have\",\"mustn't\": \"must not\",\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\"she'd\": \"she would\",\"she'd've\": \"she would have\",\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\"should've\": \"should have\",\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\"so've\": \"so have\",\"that'd\": \"that would\",\"that'd've\": \"that would have\",\n",
    "\"there'd\": \"there would\",\"there'd've\": \"there would have\",\n",
    "\"they'd\": \"they would\",\"they'd've\": \"they would have\",\"they'll\": \"they will\",\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\"they've\": \"they have\",\"to've\": \"to have\",\"wasn't\": \"was not\",\"we'd\": \"we would\",\n",
    "\"we'd've\": \"we would have\",\"we'll\": \"we will\",\"we'll've\": \"we will have\",\"we're\": \"we are\",\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\"what'll\": \"what will\",\"what'll've\": \"what will have\",\"what're\": \"what are\",\n",
    "\"what've\": \"what have\",\"when've\": \"when have\",\"where'd\": \"where did\",\n",
    "\"where've\": \"where have\",\"who'll\": \"who will\",\"who'll've\": \"who will have\",\"who've\": \"who have\",\n",
    "\"why've\": \"why have\",\"will've\": \"will have\",\"won't\": \"will not\",\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\"wouldn't\": \"would not\",\"wouldn't've\": \"would not have\",\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you would\",\"you'd've\": \"you would have\",\"you'll\": \"you will\",\"you'll've\": \"you will have\",\n",
    "\"you're\": \"you are\",\"you've\": \"you have\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular expression for finding contractions\n",
    "contractions_re=re.compile('(%s)' % '|'.join(contractions_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(text,contractions_dict=contractions_dict):\n",
    "    def replace(match):\n",
    "        return contractions_dict[match.group(0)]\n",
    "    return contractions_re.sub(replace, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes links, spaces, ecc\n",
    "def clean_text(text):\n",
    "    text=re.sub('\\w*\\d\\w*','', text)\n",
    "    text=re.sub('\\n',' ',text)\n",
    "    text=re.sub(r\"http\\S+\", \"\", text)\n",
    "    text=re.sub('[^a-z]',' ',text)\n",
    "    text=re.sub(' +',' ',text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def std_preprocess(text):\n",
    "    raw = str(text).lower()\n",
    "    raw = expand_contractions(raw)\n",
    "    raw = clean_text(raw)\n",
    "    return raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculates Terms frequency\n",
    "word_freq = dict()\n",
    "def calculate_freq_dict(texts):\n",
    "    frequency = dict()\n",
    "    for text in tqdm(texts):\n",
    "        for token in text:\n",
    "            if token in frequency:\n",
    "                frequency[token] += 1\n",
    "            else:\n",
    "                frequency[token] = 0\n",
    "    return frequency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove terms with a low frequencies\n",
    "def remove_insignificant(text, word_freq = word_freq, min_c = 2):\n",
    "    final_text = []\n",
    "    for x in text.split():\n",
    "        if word_freq[x] >= min_c:\n",
    "            final_text.append(x)\n",
    "    return \" \".join(final_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess - Stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming + Stop words if true\n",
    "def preprocess_stem(text, stop = True):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    stopped_stemmed_tokens = []\n",
    "    if stop:\n",
    "        stopped_stemmed_tokens = [p_stemmer.stem(i) for i in tokens if not i in en_stop]\n",
    "    else:\n",
    "        stopped_stemmed_tokens = [p_stemmer.stem(i) for i in tokens]\n",
    "    return \" \".join(stopped_stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_df.doc_text = docs_df.doc_text.progress_apply(std_preprocess) #apply standard preprocess\n",
    "queries_df['cl_q'] = queries_df.Query.progress_apply(std_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove word with freq < 2\n",
    "all_text = list(docs_df.doc_text.progress_apply(lambda x: x.split()))\n",
    "word_freq = calculate_freq_dict(all_text)\n",
    "docs_df.doc_text = docs_df.doc_text.progress_apply(remove_insignificant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_df['stem_text'] = docs_df.doc_text.progress_apply(preprocess_stem) #apply stemminng\n",
    "queries_df['stem_q'] = queries_df.cl_q.progress_apply(lambda x: preprocess_stem(x, False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process - Lem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm',disable=['ner','parser'])\n",
    "nlp.max_length=5000000\n",
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization + Stop words if true\n",
    "def preprocess_lem(text, stop = True):\n",
    "    if stop:\n",
    "        stopped_lemmed_tokens = ' '.join([token.lemma_ for token in list(nlp(str(text))) if (token.is_stop==False)])\n",
    "    else:\n",
    "        stopped_lemmed_tokens = ' '.join([token.lemma_ for token in list(nlp(str(text)))])\n",
    "    return stopped_lemmed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_df['lem_text'] = docs_df.doc_text.progress_apply(preprocess_lem)\n",
    "queries_df['lem_q'] = queries_df.cl_q.progress_apply(lambda x: preprocess_lem(x, False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Preprocessed Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_df.to_csv(save_docs, index = False)\n",
    "queries_df.to_csv(save_queries, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply preprocess to test_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('Docs/test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_docs = pd.read_csv('ProcDocs/Split_0.2/docs_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_lucene = test_data[['Query_number', 'doc_number']].copy().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_query = test_data[['Query_number', 'Query']].copy().reset_index(drop=True)\n",
    "test_data_query.Query = test_data_query.Query.apply(str)\n",
    "test_data_query.Query_number = test_data_query.Query_number.apply(int)\n",
    "test_data_query = test_data_query.drop_duplicates('Query_number').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_doc = test_data[['doc_number', 'doc_text']].copy().reset_index(drop=True)\n",
    "test_data_doc.doc_text = test_data_doc.doc_text.apply(str)\n",
    "test_data_doc.doc_number = test_data_doc.doc_number.apply(int)\n",
    "test_data_doc = test_data_doc.drop_duplicates('doc_number').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_doc = test_data_doc[~(test_data_doc.doc_number.isin(list(train_docs.doc_number.unique())))].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_query.Query = test_data_query.Query.progress_apply(std_preprocess)\n",
    "test_data_doc.doc_text = test_data_doc.doc_text.progress_apply(std_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove word with freq < 2 normal text\n",
    "all_text = list(test_data_doc.doc_text.progress_apply(lambda x: x.split()))\n",
    "word_freq = calculate_freq_dict(all_text)\n",
    "test_data_doc.doc_text = test_data_doc.doc_text.progress_apply(lambda x: remove_insignificant(x, word_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_doc['lem_text'] = test_data_doc.doc_text.progress_apply(preprocess_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_query['lem_q'] = test_data_query.Query.progress_apply(lambda x: preprocess_lem(x, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_doc.to_csv(\"ProcDocs/test_data/test_data_doc.csv\", index = False)\n",
    "test_data_query.to_csv(\"ProcDocs/test_data/test_data_query.csv\", index = False)\n",
    "test_data_lucene.to_csv(\"ProcDocs/test_data/test_data_lucene.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "19649b6d477f04954267d7dfcc1e3219afd53992c8847ec6a09d5cd5145b7914"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
